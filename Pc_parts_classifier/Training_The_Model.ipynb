{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=ImageFolder(root='pc_parts_ready',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader(dataset=data,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=24, kernel_size=4, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=53*53*24, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=200)\n",
    "        self.fc3 = nn.Linear(in_features=200, out_features=80)\n",
    "        self.fc4 = nn.Linear(in_features=80, out_features=14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1,53*53*24)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 3043\n",
       "    Root location: pc_parts_ready\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 24, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=67416, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=80, bias=True)\n",
       "  (fc4): Linear(in_features=80, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=CNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.6343867778778076 Batch : 1 Epoch : 1/15\n",
      "Loss : 2.614001989364624 Batch : 2 Epoch : 1/15\n",
      "Loss : 2.673722743988037 Batch : 3 Epoch : 1/15\n",
      "Loss : 2.667572021484375 Batch : 4 Epoch : 1/15\n",
      "Loss : 2.5885815620422363 Batch : 5 Epoch : 1/15\n",
      "Loss : 2.6117639541625977 Batch : 6 Epoch : 1/15\n",
      "Loss : 2.5866594314575195 Batch : 7 Epoch : 1/15\n",
      "Loss : 2.6487998962402344 Batch : 8 Epoch : 1/15\n",
      "Loss : 2.5797550678253174 Batch : 9 Epoch : 1/15\n",
      "Loss : 2.5793960094451904 Batch : 10 Epoch : 1/15\n",
      "Loss : 2.6324901580810547 Batch : 11 Epoch : 1/15\n",
      "Loss : 2.61806321144104 Batch : 12 Epoch : 1/15\n",
      "Loss : 2.610595464706421 Batch : 13 Epoch : 1/15\n",
      "Loss : 2.6325573921203613 Batch : 14 Epoch : 1/15\n",
      "Loss : 2.635603189468384 Batch : 15 Epoch : 1/15\n",
      "Loss : 2.578911304473877 Batch : 16 Epoch : 1/15\n",
      "Loss : 2.6045172214508057 Batch : 17 Epoch : 1/15\n",
      "Loss : 2.606374502182007 Batch : 18 Epoch : 1/15\n",
      "Loss : 2.5894076824188232 Batch : 19 Epoch : 1/15\n",
      "Loss : 2.562462329864502 Batch : 20 Epoch : 1/15\n",
      "Loss : 2.5853233337402344 Batch : 21 Epoch : 1/15\n",
      "Loss : 2.576425790786743 Batch : 22 Epoch : 1/15\n",
      "Loss : 2.548549175262451 Batch : 23 Epoch : 1/15\n",
      "Loss : 2.5664849281311035 Batch : 24 Epoch : 1/15\n",
      "Loss : 2.4783942699432373 Batch : 1 Epoch : 2/15\n",
      "Loss : 2.5311999320983887 Batch : 2 Epoch : 2/15\n",
      "Loss : 2.571559190750122 Batch : 3 Epoch : 2/15\n",
      "Loss : 2.46317720413208 Batch : 4 Epoch : 2/15\n",
      "Loss : 2.491238832473755 Batch : 5 Epoch : 2/15\n",
      "Loss : 2.4887263774871826 Batch : 6 Epoch : 2/15\n",
      "Loss : 2.495206356048584 Batch : 7 Epoch : 2/15\n",
      "Loss : 2.4836602210998535 Batch : 8 Epoch : 2/15\n",
      "Loss : 2.5325818061828613 Batch : 9 Epoch : 2/15\n",
      "Loss : 2.4862897396087646 Batch : 10 Epoch : 2/15\n",
      "Loss : 2.4486615657806396 Batch : 11 Epoch : 2/15\n",
      "Loss : 2.5500669479370117 Batch : 12 Epoch : 2/15\n",
      "Loss : 2.4621458053588867 Batch : 13 Epoch : 2/15\n",
      "Loss : 2.475527286529541 Batch : 14 Epoch : 2/15\n",
      "Loss : 2.563504934310913 Batch : 15 Epoch : 2/15\n",
      "Loss : 2.4475257396698 Batch : 16 Epoch : 2/15\n",
      "Loss : 2.434567928314209 Batch : 17 Epoch : 2/15\n",
      "Loss : 2.4810643196105957 Batch : 18 Epoch : 2/15\n",
      "Loss : 2.4412684440612793 Batch : 19 Epoch : 2/15\n",
      "Loss : 2.4212875366210938 Batch : 20 Epoch : 2/15\n",
      "Loss : 2.5620946884155273 Batch : 21 Epoch : 2/15\n",
      "Loss : 2.414376974105835 Batch : 22 Epoch : 2/15\n",
      "Loss : 2.3770270347595215 Batch : 23 Epoch : 2/15\n",
      "Loss : 2.3796427249908447 Batch : 24 Epoch : 2/15\n",
      "Loss : 2.3665335178375244 Batch : 1 Epoch : 3/15\n",
      "Loss : 2.4273734092712402 Batch : 2 Epoch : 3/15\n",
      "Loss : 2.2570717334747314 Batch : 3 Epoch : 3/15\n",
      "Loss : 2.315028667449951 Batch : 4 Epoch : 3/15\n",
      "Loss : 2.2558374404907227 Batch : 5 Epoch : 3/15\n",
      "Loss : 2.355679512023926 Batch : 6 Epoch : 3/15\n",
      "Loss : 2.2695348262786865 Batch : 7 Epoch : 3/15\n",
      "Loss : 2.3462774753570557 Batch : 8 Epoch : 3/15\n",
      "Loss : 2.2873246669769287 Batch : 9 Epoch : 3/15\n",
      "Loss : 2.2360947132110596 Batch : 10 Epoch : 3/15\n",
      "Loss : 2.375206708908081 Batch : 11 Epoch : 3/15\n",
      "Loss : 2.4013984203338623 Batch : 12 Epoch : 3/15\n",
      "Loss : 2.463373899459839 Batch : 13 Epoch : 3/15\n",
      "Loss : 2.367985725402832 Batch : 14 Epoch : 3/15\n",
      "Loss : 2.3217620849609375 Batch : 15 Epoch : 3/15\n",
      "Loss : 2.2670998573303223 Batch : 16 Epoch : 3/15\n",
      "Loss : 2.234929323196411 Batch : 17 Epoch : 3/15\n",
      "Loss : 2.2386746406555176 Batch : 18 Epoch : 3/15\n",
      "Loss : 2.2709622383117676 Batch : 19 Epoch : 3/15\n",
      "Loss : 2.286311626434326 Batch : 20 Epoch : 3/15\n",
      "Loss : 2.1694235801696777 Batch : 21 Epoch : 3/15\n",
      "Loss : 2.2808451652526855 Batch : 22 Epoch : 3/15\n",
      "Loss : 2.2283458709716797 Batch : 23 Epoch : 3/15\n",
      "Loss : 2.1907765865325928 Batch : 24 Epoch : 3/15\n",
      "Loss : 2.2522852420806885 Batch : 1 Epoch : 4/15\n",
      "Loss : 2.1360902786254883 Batch : 2 Epoch : 4/15\n",
      "Loss : 2.0943491458892822 Batch : 3 Epoch : 4/15\n",
      "Loss : 1.9862608909606934 Batch : 4 Epoch : 4/15\n",
      "Loss : 2.1738853454589844 Batch : 5 Epoch : 4/15\n",
      "Loss : 2.2521018981933594 Batch : 6 Epoch : 4/15\n",
      "Loss : 1.975799322128296 Batch : 7 Epoch : 4/15\n",
      "Loss : 2.1810109615325928 Batch : 8 Epoch : 4/15\n",
      "Loss : 2.1620824337005615 Batch : 9 Epoch : 4/15\n",
      "Loss : 2.024324655532837 Batch : 10 Epoch : 4/15\n",
      "Loss : 2.1796345710754395 Batch : 11 Epoch : 4/15\n",
      "Loss : 2.023606061935425 Batch : 12 Epoch : 4/15\n",
      "Loss : 2.123371124267578 Batch : 13 Epoch : 4/15\n",
      "Loss : 2.0061898231506348 Batch : 14 Epoch : 4/15\n",
      "Loss : 2.154930591583252 Batch : 15 Epoch : 4/15\n",
      "Loss : 2.086568832397461 Batch : 16 Epoch : 4/15\n",
      "Loss : 2.0222585201263428 Batch : 17 Epoch : 4/15\n",
      "Loss : 2.045485258102417 Batch : 18 Epoch : 4/15\n",
      "Loss : 2.070944309234619 Batch : 19 Epoch : 4/15\n",
      "Loss : 1.9103567600250244 Batch : 20 Epoch : 4/15\n",
      "Loss : 2.061406373977661 Batch : 21 Epoch : 4/15\n",
      "Loss : 1.9693399667739868 Batch : 22 Epoch : 4/15\n",
      "Loss : 1.902872920036316 Batch : 23 Epoch : 4/15\n",
      "Loss : 1.932773470878601 Batch : 24 Epoch : 4/15\n",
      "Loss : 1.7649905681610107 Batch : 1 Epoch : 5/15\n",
      "Loss : 2.120885133743286 Batch : 2 Epoch : 5/15\n",
      "Loss : 1.7623274326324463 Batch : 3 Epoch : 5/15\n",
      "Loss : 1.7327089309692383 Batch : 4 Epoch : 5/15\n",
      "Loss : 1.849034070968628 Batch : 5 Epoch : 5/15\n",
      "Loss : 1.8845854997634888 Batch : 6 Epoch : 5/15\n",
      "Loss : 1.802024245262146 Batch : 7 Epoch : 5/15\n",
      "Loss : 2.033658742904663 Batch : 8 Epoch : 5/15\n",
      "Loss : 1.7994663715362549 Batch : 9 Epoch : 5/15\n",
      "Loss : 1.6753016710281372 Batch : 10 Epoch : 5/15\n",
      "Loss : 1.7169554233551025 Batch : 11 Epoch : 5/15\n",
      "Loss : 1.7253437042236328 Batch : 12 Epoch : 5/15\n",
      "Loss : 1.8685028553009033 Batch : 13 Epoch : 5/15\n",
      "Loss : 1.8809962272644043 Batch : 14 Epoch : 5/15\n",
      "Loss : 1.9066566228866577 Batch : 15 Epoch : 5/15\n",
      "Loss : 1.7522838115692139 Batch : 16 Epoch : 5/15\n",
      "Loss : 1.7229901552200317 Batch : 17 Epoch : 5/15\n",
      "Loss : 1.7198731899261475 Batch : 18 Epoch : 5/15\n",
      "Loss : 1.7593226432800293 Batch : 19 Epoch : 5/15\n",
      "Loss : 1.7523348331451416 Batch : 20 Epoch : 5/15\n",
      "Loss : 1.5468499660491943 Batch : 21 Epoch : 5/15\n",
      "Loss : 1.6787358522415161 Batch : 22 Epoch : 5/15\n",
      "Loss : 1.7451366186141968 Batch : 23 Epoch : 5/15\n",
      "Loss : 1.8487415313720703 Batch : 24 Epoch : 5/15\n",
      "Loss : 1.3405290842056274 Batch : 1 Epoch : 6/15\n",
      "Loss : 1.5304487943649292 Batch : 2 Epoch : 6/15\n",
      "Loss : 1.4578619003295898 Batch : 3 Epoch : 6/15\n",
      "Loss : 1.5737073421478271 Batch : 4 Epoch : 6/15\n",
      "Loss : 1.491411805152893 Batch : 5 Epoch : 6/15\n",
      "Loss : 1.3975733518600464 Batch : 6 Epoch : 6/15\n",
      "Loss : 1.4336910247802734 Batch : 7 Epoch : 6/15\n",
      "Loss : 1.3745698928833008 Batch : 8 Epoch : 6/15\n",
      "Loss : 1.4114466905593872 Batch : 9 Epoch : 6/15\n",
      "Loss : 1.3757292032241821 Batch : 10 Epoch : 6/15\n",
      "Loss : 1.3077422380447388 Batch : 11 Epoch : 6/15\n",
      "Loss : 1.3649158477783203 Batch : 12 Epoch : 6/15\n",
      "Loss : 1.2761965990066528 Batch : 13 Epoch : 6/15\n",
      "Loss : 1.4335435628890991 Batch : 14 Epoch : 6/15\n",
      "Loss : 1.512784719467163 Batch : 15 Epoch : 6/15\n",
      "Loss : 1.2171562910079956 Batch : 16 Epoch : 6/15\n",
      "Loss : 1.170667052268982 Batch : 17 Epoch : 6/15\n",
      "Loss : 1.2984821796417236 Batch : 18 Epoch : 6/15\n",
      "Loss : 1.3393062353134155 Batch : 19 Epoch : 6/15\n",
      "Loss : 1.2203943729400635 Batch : 20 Epoch : 6/15\n",
      "Loss : 1.2377514839172363 Batch : 21 Epoch : 6/15\n",
      "Loss : 1.296350121498108 Batch : 22 Epoch : 6/15\n",
      "Loss : 1.3629274368286133 Batch : 23 Epoch : 6/15\n",
      "Loss : 1.1929556131362915 Batch : 24 Epoch : 6/15\n",
      "Loss : 1.1297959089279175 Batch : 1 Epoch : 7/15\n",
      "Loss : 0.9767031073570251 Batch : 2 Epoch : 7/15\n",
      "Loss : 1.1255215406417847 Batch : 3 Epoch : 7/15\n",
      "Loss : 0.9160232543945312 Batch : 4 Epoch : 7/15\n",
      "Loss : 1.009581208229065 Batch : 5 Epoch : 7/15\n",
      "Loss : 0.9930189251899719 Batch : 6 Epoch : 7/15\n",
      "Loss : 0.7547702789306641 Batch : 7 Epoch : 7/15\n",
      "Loss : 0.9014989137649536 Batch : 8 Epoch : 7/15\n",
      "Loss : 0.9861780405044556 Batch : 9 Epoch : 7/15\n",
      "Loss : 1.0998814105987549 Batch : 10 Epoch : 7/15\n",
      "Loss : 0.7112646698951721 Batch : 11 Epoch : 7/15\n",
      "Loss : 0.965173602104187 Batch : 12 Epoch : 7/15\n",
      "Loss : 0.9874931573867798 Batch : 13 Epoch : 7/15\n",
      "Loss : 0.8127716183662415 Batch : 14 Epoch : 7/15\n",
      "Loss : 0.9115992784500122 Batch : 15 Epoch : 7/15\n",
      "Loss : 0.9601604342460632 Batch : 16 Epoch : 7/15\n",
      "Loss : 0.7927706241607666 Batch : 17 Epoch : 7/15\n",
      "Loss : 0.9943485856056213 Batch : 18 Epoch : 7/15\n",
      "Loss : 0.8103405237197876 Batch : 19 Epoch : 7/15\n",
      "Loss : 0.9417069554328918 Batch : 20 Epoch : 7/15\n",
      "Loss : 1.0701968669891357 Batch : 21 Epoch : 7/15\n",
      "Loss : 0.9190037846565247 Batch : 22 Epoch : 7/15\n",
      "Loss : 0.8406755328178406 Batch : 23 Epoch : 7/15\n",
      "Loss : 0.8827769160270691 Batch : 24 Epoch : 7/15\n",
      "Loss : 0.5141229033470154 Batch : 1 Epoch : 8/15\n",
      "Loss : 0.6198623180389404 Batch : 2 Epoch : 8/15\n",
      "Loss : 0.5414513945579529 Batch : 3 Epoch : 8/15\n",
      "Loss : 0.5473635196685791 Batch : 4 Epoch : 8/15\n",
      "Loss : 0.5506592392921448 Batch : 5 Epoch : 8/15\n",
      "Loss : 0.525347888469696 Batch : 6 Epoch : 8/15\n",
      "Loss : 0.5504893660545349 Batch : 7 Epoch : 8/15\n",
      "Loss : 0.4110047221183777 Batch : 8 Epoch : 8/15\n",
      "Loss : 0.6451956629753113 Batch : 9 Epoch : 8/15\n",
      "Loss : 0.6686018705368042 Batch : 10 Epoch : 8/15\n",
      "Loss : 0.6116378307342529 Batch : 11 Epoch : 8/15\n",
      "Loss : 0.5531328916549683 Batch : 12 Epoch : 8/15\n",
      "Loss : 0.3959740102291107 Batch : 13 Epoch : 8/15\n",
      "Loss : 0.5830270648002625 Batch : 14 Epoch : 8/15\n",
      "Loss : 0.7349951267242432 Batch : 15 Epoch : 8/15\n",
      "Loss : 0.42371684312820435 Batch : 16 Epoch : 8/15\n",
      "Loss : 0.47539854049682617 Batch : 17 Epoch : 8/15\n",
      "Loss : 0.526911199092865 Batch : 18 Epoch : 8/15\n",
      "Loss : 0.4703060984611511 Batch : 19 Epoch : 8/15\n",
      "Loss : 0.559872031211853 Batch : 20 Epoch : 8/15\n",
      "Loss : 0.6932362914085388 Batch : 21 Epoch : 8/15\n",
      "Loss : 0.4698350429534912 Batch : 22 Epoch : 8/15\n",
      "Loss : 0.4592495560646057 Batch : 23 Epoch : 8/15\n",
      "Loss : 0.5682085752487183 Batch : 24 Epoch : 8/15\n",
      "Loss : 0.4034191071987152 Batch : 1 Epoch : 9/15\n",
      "Loss : 0.4467528462409973 Batch : 2 Epoch : 9/15\n",
      "Loss : 0.27064967155456543 Batch : 3 Epoch : 9/15\n",
      "Loss : 0.40284907817840576 Batch : 4 Epoch : 9/15\n",
      "Loss : 0.3686281144618988 Batch : 5 Epoch : 9/15\n",
      "Loss : 0.2704549729824066 Batch : 6 Epoch : 9/15\n",
      "Loss : 0.34850361943244934 Batch : 7 Epoch : 9/15\n",
      "Loss : 0.2949521243572235 Batch : 8 Epoch : 9/15\n",
      "Loss : 0.37495094537734985 Batch : 9 Epoch : 9/15\n",
      "Loss : 0.374398797750473 Batch : 10 Epoch : 9/15\n",
      "Loss : 0.28602299094200134 Batch : 11 Epoch : 9/15\n",
      "Loss : 0.30201321840286255 Batch : 12 Epoch : 9/15\n",
      "Loss : 0.3964543342590332 Batch : 13 Epoch : 9/15\n",
      "Loss : 0.2735474407672882 Batch : 14 Epoch : 9/15\n",
      "Loss : 0.32094818353652954 Batch : 15 Epoch : 9/15\n",
      "Loss : 0.3257371783256531 Batch : 16 Epoch : 9/15\n",
      "Loss : 0.42107948660850525 Batch : 17 Epoch : 9/15\n",
      "Loss : 0.21930155158042908 Batch : 18 Epoch : 9/15\n",
      "Loss : 0.2096906155347824 Batch : 19 Epoch : 9/15\n",
      "Loss : 0.2418019026517868 Batch : 20 Epoch : 9/15\n",
      "Loss : 0.35665684938430786 Batch : 21 Epoch : 9/15\n",
      "Loss : 0.28859758377075195 Batch : 22 Epoch : 9/15\n",
      "Loss : 0.3144284784793854 Batch : 23 Epoch : 9/15\n",
      "Loss : 0.2799015939235687 Batch : 24 Epoch : 9/15\n",
      "Loss : 0.13768798112869263 Batch : 1 Epoch : 10/15\n",
      "Loss : 0.21087516844272614 Batch : 2 Epoch : 10/15\n",
      "Loss : 0.13036611676216125 Batch : 3 Epoch : 10/15\n",
      "Loss : 0.13261385262012482 Batch : 4 Epoch : 10/15\n",
      "Loss : 0.16626760363578796 Batch : 5 Epoch : 10/15\n",
      "Loss : 0.254864364862442 Batch : 6 Epoch : 10/15\n",
      "Loss : 0.16576127707958221 Batch : 7 Epoch : 10/15\n",
      "Loss : 0.16583433747291565 Batch : 8 Epoch : 10/15\n",
      "Loss : 0.17578421533107758 Batch : 9 Epoch : 10/15\n",
      "Loss : 0.10690760612487793 Batch : 10 Epoch : 10/15\n",
      "Loss : 0.17859849333763123 Batch : 11 Epoch : 10/15\n",
      "Loss : 0.15288278460502625 Batch : 12 Epoch : 10/15\n",
      "Loss : 0.123298779129982 Batch : 13 Epoch : 10/15\n",
      "Loss : 0.14776676893234253 Batch : 14 Epoch : 10/15\n",
      "Loss : 0.2955029308795929 Batch : 15 Epoch : 10/15\n",
      "Loss : 0.11718175560235977 Batch : 16 Epoch : 10/15\n",
      "Loss : 0.32368898391723633 Batch : 17 Epoch : 10/15\n",
      "Loss : 0.12440289556980133 Batch : 18 Epoch : 10/15\n",
      "Loss : 0.26265135407447815 Batch : 19 Epoch : 10/15\n",
      "Loss : 0.3219663202762604 Batch : 20 Epoch : 10/15\n",
      "Loss : 0.16482672095298767 Batch : 21 Epoch : 10/15\n",
      "Loss : 0.21860450506210327 Batch : 22 Epoch : 10/15\n",
      "Loss : 0.2740347683429718 Batch : 23 Epoch : 10/15\n",
      "Loss : 0.20372898876667023 Batch : 24 Epoch : 10/15\n",
      "Loss : 0.10258542746305466 Batch : 1 Epoch : 11/15\n",
      "Loss : 0.12778471410274506 Batch : 2 Epoch : 11/15\n",
      "Loss : 0.16074904799461365 Batch : 3 Epoch : 11/15\n",
      "Loss : 0.1304548680782318 Batch : 4 Epoch : 11/15\n",
      "Loss : 0.08514078706502914 Batch : 5 Epoch : 11/15\n",
      "Loss : 0.08837812393903732 Batch : 6 Epoch : 11/15\n",
      "Loss : 0.2051153928041458 Batch : 7 Epoch : 11/15\n",
      "Loss : 0.25522658228874207 Batch : 8 Epoch : 11/15\n",
      "Loss : 0.17396393418312073 Batch : 9 Epoch : 11/15\n",
      "Loss : 0.09154301881790161 Batch : 10 Epoch : 11/15\n",
      "Loss : 0.14076842367649078 Batch : 11 Epoch : 11/15\n",
      "Loss : 0.08432639390230179 Batch : 12 Epoch : 11/15\n",
      "Loss : 0.2159011960029602 Batch : 13 Epoch : 11/15\n",
      "Loss : 0.22932928800582886 Batch : 14 Epoch : 11/15\n",
      "Loss : 0.16005276143550873 Batch : 15 Epoch : 11/15\n",
      "Loss : 0.19664788246154785 Batch : 16 Epoch : 11/15\n",
      "Loss : 0.3335685431957245 Batch : 17 Epoch : 11/15\n",
      "Loss : 0.14635154604911804 Batch : 18 Epoch : 11/15\n",
      "Loss : 0.06973666697740555 Batch : 19 Epoch : 11/15\n",
      "Loss : 0.2245616316795349 Batch : 20 Epoch : 11/15\n",
      "Loss : 0.20068767666816711 Batch : 21 Epoch : 11/15\n",
      "Loss : 0.17288601398468018 Batch : 22 Epoch : 11/15\n",
      "Loss : 0.09804832935333252 Batch : 23 Epoch : 11/15\n",
      "Loss : 0.2102912813425064 Batch : 24 Epoch : 11/15\n",
      "Loss : 0.10171736776828766 Batch : 1 Epoch : 12/15\n",
      "Loss : 0.1379120945930481 Batch : 2 Epoch : 12/15\n",
      "Loss : 0.15837784111499786 Batch : 3 Epoch : 12/15\n",
      "Loss : 0.08995943516492844 Batch : 4 Epoch : 12/15\n",
      "Loss : 0.15445443987846375 Batch : 5 Epoch : 12/15\n",
      "Loss : 0.15930667519569397 Batch : 6 Epoch : 12/15\n",
      "Loss : 0.0719541609287262 Batch : 7 Epoch : 12/15\n",
      "Loss : 0.0572296679019928 Batch : 8 Epoch : 12/15\n",
      "Loss : 0.08284205943346024 Batch : 9 Epoch : 12/15\n",
      "Loss : 0.07940609008073807 Batch : 10 Epoch : 12/15\n",
      "Loss : 0.08333394676446915 Batch : 11 Epoch : 12/15\n",
      "Loss : 0.1513908952474594 Batch : 12 Epoch : 12/15\n",
      "Loss : 0.11692969501018524 Batch : 13 Epoch : 12/15\n",
      "Loss : 0.08303322643041611 Batch : 14 Epoch : 12/15\n",
      "Loss : 0.24961286783218384 Batch : 15 Epoch : 12/15\n",
      "Loss : 0.10157451778650284 Batch : 16 Epoch : 12/15\n",
      "Loss : 0.061497047543525696 Batch : 17 Epoch : 12/15\n",
      "Loss : 0.09787292033433914 Batch : 18 Epoch : 12/15\n",
      "Loss : 0.2333899289369583 Batch : 19 Epoch : 12/15\n",
      "Loss : 0.12674717605113983 Batch : 20 Epoch : 12/15\n",
      "Loss : 0.309866338968277 Batch : 21 Epoch : 12/15\n",
      "Loss : 0.12811315059661865 Batch : 22 Epoch : 12/15\n",
      "Loss : 0.1300199031829834 Batch : 23 Epoch : 12/15\n",
      "Loss : 0.06406043469905853 Batch : 24 Epoch : 12/15\n",
      "Loss : 0.05916908010840416 Batch : 1 Epoch : 13/15\n",
      "Loss : 0.10096470266580582 Batch : 2 Epoch : 13/15\n",
      "Loss : 0.05482601374387741 Batch : 3 Epoch : 13/15\n",
      "Loss : 0.03422640636563301 Batch : 4 Epoch : 13/15\n",
      "Loss : 0.02623290941119194 Batch : 5 Epoch : 13/15\n",
      "Loss : 0.08705402910709381 Batch : 6 Epoch : 13/15\n",
      "Loss : 0.11611184477806091 Batch : 7 Epoch : 13/15\n",
      "Loss : 0.035516105592250824 Batch : 8 Epoch : 13/15\n",
      "Loss : 0.055334754288196564 Batch : 9 Epoch : 13/15\n",
      "Loss : 0.075041264295578 Batch : 10 Epoch : 13/15\n",
      "Loss : 0.037574298679828644 Batch : 11 Epoch : 13/15\n",
      "Loss : 0.16339126229286194 Batch : 12 Epoch : 13/15\n",
      "Loss : 0.11170046776533127 Batch : 13 Epoch : 13/15\n",
      "Loss : 0.09375924617052078 Batch : 14 Epoch : 13/15\n",
      "Loss : 0.0791751965880394 Batch : 15 Epoch : 13/15\n",
      "Loss : 0.03454270586371422 Batch : 16 Epoch : 13/15\n",
      "Loss : 0.23962856829166412 Batch : 17 Epoch : 13/15\n",
      "Loss : 0.1820213794708252 Batch : 18 Epoch : 13/15\n",
      "Loss : 0.0726696103811264 Batch : 19 Epoch : 13/15\n",
      "Loss : 0.19364891946315765 Batch : 20 Epoch : 13/15\n",
      "Loss : 0.2157047688961029 Batch : 21 Epoch : 13/15\n",
      "Loss : 0.10535640269517899 Batch : 22 Epoch : 13/15\n",
      "Loss : 0.10408639162778854 Batch : 23 Epoch : 13/15\n",
      "Loss : 0.25169938802719116 Batch : 24 Epoch : 13/15\n",
      "Loss : 0.028676029294729233 Batch : 1 Epoch : 14/15\n",
      "Loss : 0.071656234562397 Batch : 2 Epoch : 14/15\n",
      "Loss : 0.024295758455991745 Batch : 3 Epoch : 14/15\n",
      "Loss : 0.05297426879405975 Batch : 4 Epoch : 14/15\n",
      "Loss : 0.07020770758390427 Batch : 5 Epoch : 14/15\n",
      "Loss : 0.05087995156645775 Batch : 6 Epoch : 14/15\n",
      "Loss : 0.14073146879673004 Batch : 7 Epoch : 14/15\n",
      "Loss : 0.0982903316617012 Batch : 8 Epoch : 14/15\n",
      "Loss : 0.12664824724197388 Batch : 9 Epoch : 14/15\n",
      "Loss : 0.05512985214591026 Batch : 10 Epoch : 14/15\n",
      "Loss : 0.08810398727655411 Batch : 11 Epoch : 14/15\n",
      "Loss : 0.05585341155529022 Batch : 12 Epoch : 14/15\n",
      "Loss : 0.1005614846944809 Batch : 13 Epoch : 14/15\n",
      "Loss : 0.14404958486557007 Batch : 14 Epoch : 14/15\n",
      "Loss : 0.13128386437892914 Batch : 15 Epoch : 14/15\n",
      "Loss : 0.08306262642145157 Batch : 16 Epoch : 14/15\n",
      "Loss : 0.08624627441167831 Batch : 17 Epoch : 14/15\n",
      "Loss : 0.0494314469397068 Batch : 18 Epoch : 14/15\n",
      "Loss : 0.041475534439086914 Batch : 19 Epoch : 14/15\n",
      "Loss : 0.0846579298377037 Batch : 20 Epoch : 14/15\n",
      "Loss : 0.17611239850521088 Batch : 21 Epoch : 14/15\n",
      "Loss : 0.0599413625895977 Batch : 22 Epoch : 14/15\n",
      "Loss : 0.10140062123537064 Batch : 23 Epoch : 14/15\n",
      "Loss : 0.22352440655231476 Batch : 24 Epoch : 14/15\n",
      "Loss : 0.058361656963825226 Batch : 1 Epoch : 15/15\n",
      "Loss : 0.03935316950082779 Batch : 2 Epoch : 15/15\n",
      "Loss : 0.12931816279888153 Batch : 3 Epoch : 15/15\n",
      "Loss : 0.06428831815719604 Batch : 4 Epoch : 15/15\n",
      "Loss : 0.03953270986676216 Batch : 5 Epoch : 15/15\n",
      "Loss : 0.06918944418430328 Batch : 6 Epoch : 15/15\n",
      "Loss : 0.042689770460128784 Batch : 7 Epoch : 15/15\n",
      "Loss : 0.11621887236833572 Batch : 8 Epoch : 15/15\n",
      "Loss : 0.05113128200173378 Batch : 9 Epoch : 15/15\n",
      "Loss : 0.038676466792821884 Batch : 10 Epoch : 15/15\n",
      "Loss : 0.07734046876430511 Batch : 11 Epoch : 15/15\n",
      "Loss : 0.06478483974933624 Batch : 12 Epoch : 15/15\n",
      "Loss : 0.08176286518573761 Batch : 13 Epoch : 15/15\n",
      "Loss : 0.13516148924827576 Batch : 14 Epoch : 15/15\n",
      "Loss : 0.06208915263414383 Batch : 15 Epoch : 15/15\n",
      "Loss : 0.2233639359474182 Batch : 16 Epoch : 15/15\n",
      "Loss : 0.08417648822069168 Batch : 17 Epoch : 15/15\n",
      "Loss : 0.12841381132602692 Batch : 18 Epoch : 15/15\n",
      "Loss : 0.12265652418136597 Batch : 19 Epoch : 15/15\n",
      "Loss : 0.05382635444402695 Batch : 20 Epoch : 15/15\n",
      "Loss : 0.2532040774822235 Batch : 21 Epoch : 15/15\n",
      "Loss : 0.11044640839099884 Batch : 22 Epoch : 15/15\n",
      "Loss : 0.04401908069849014 Batch : 23 Epoch : 15/15\n",
      "Loss : 0.053764842450618744 Batch : 24 Epoch : 15/15\n"
     ]
    }
   ],
   "source": [
    "epochs=15  \n",
    "for epoch in range(epochs):\n",
    "    for b,(x,y) in enumerate(dataloader):\n",
    "        y_pred=model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss : {loss} Batch : {b+1} Epoch : {epoch+1}/{epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model_weigths.pth\")\n",
    "torch.save(model,'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 24, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=67416, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=80, bias=True)\n",
       "  (fc4): Linear(in_features=80, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=torch.load('model.pth')\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
